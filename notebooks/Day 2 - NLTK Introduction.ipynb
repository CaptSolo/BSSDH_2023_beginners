{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXReOs-4Kh7w"
   },
   "source": [
    "## Natural Language Toolkit (NLTK)\n",
    "\n",
    "**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to [over 50 corpora and lexical resources](http://www.nltk.org/nltk_data/) such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "http://www.nltk.org/\n",
    "\n",
    "NLTK library documentation (reference) = *Use it to look up how to use a particular NLTK library function*\n",
    "* https://www.nltk.org/api/nltk.html\n",
    "\n",
    "---\n",
    "\n",
    "NLTK wiki (collaboratively edited documentation):\n",
    "* https://github.com/nltk/nltk/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q4oMz88Kh71"
   },
   "source": [
    "### Book: Natural Language Processing with Python \n",
    "\n",
    "NLTK book provides a practical introduction to programming for language processing.\n",
    "\n",
    "Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more.\n",
    "\n",
    "Online: http://www.nltk.org/book/\n",
    "\n",
    "* we will start with Chapter 1: [\"Language Processing and Python\"](http://www.nltk.org/book/ch01.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started - Jupyter notebook\n",
    "\n",
    "## https://bit.ly/bssdh_2023_python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this Jupyter notebook (`Day 2 - NLTK Introduction`) on Github: \n",
    "- https://github.com/CaptSolo/BSSDH_2023_beginners/tree/main/notebooks\n",
    "\n",
    "Download the notebook file to your computer: click the \"Download raw file\" button.\n",
    "\n",
    "![Download button](https://github.com/CaptSolo/BSSDH_2023_beginners/blob/main/notebooks/img/download_button.png?raw=1)\n",
    "\n",
    "Open [Google Colab](https://colab.research.google.com/), choose the \"Upload\" tab and upload the downloaded notebook file.\n",
    "\n",
    "* Uploaded notebooks can be found in the [Google Drive](https://drive.google.com/) folder `Colab Notebooks`\n",
    "\n",
    "You are now ready for the workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c211Je63Kh73"
   },
   "source": [
    "## 1) Getting started - NLTK\n",
    "\n",
    "NLTK book: http://www.nltk.org/book/ch01.html#getting-started-with-nltk\n",
    "\n",
    "* Loading NLTK (Python module)\n",
    "* Downloading NLTK language resources (corpora, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3wVnd6JKh73"
   },
   "outputs": [],
   "source": [
    "# In order to use a Python library, we need to import (load) it\n",
    "\n",
    "import nltk\n",
    "import pandas as pd # we will use it to read our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IiDmy3H4Kh74",
    "outputId": "f70c25bd-cd9e-423c-a2cb-6c8540bccaf3"
   },
   "outputs": [],
   "source": [
    "# Let's check what NLTK version we have (for easier troubleshooting and reproducibility)\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P25KGyTKh76"
   },
   "source": [
    "### nltk.Text\n",
    "\n",
    "**`ntlk.Text` is a simple NLTK helper for loading and exploring textual content (a sequence of words / string tokens):**\n",
    "\n",
    "... intended to support initial exploration of texts (via the interactive console). It can perform a variety of analyses on the text’s contexts (e.g., counting, concordancing, collocation discovery), and display the results.\n",
    "\n",
    "Documentation: [nltk.Text](https://www.nltk.org/api/nltk.html#nltk.text.Text)\n",
    "* lists what we can do with text once it is loaded into nltk.Text(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGazEviJKh77",
    "outputId": "7f309be8-3086-49d2-e615-763111f2e39c"
   },
   "outputs": [],
   "source": [
    "# Now we can try a simple example:\n",
    "\n",
    "my_word_list = [\"This\", \"is\", \"just\", \"an\", \"example\", \"Another\", \"example\", \"here\"]\n",
    "my_text = nltk.Text(my_word_list)\n",
    "\n",
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTbrL4BbN5A_",
    "outputId": "4b2db18d-9fd1-4df6-ce0a-06a9351941ab"
   },
   "outputs": [],
   "source": [
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LInOevQ9Kh77",
    "outputId": "57560680-1aea-4823-f62e-014446c4c742"
   },
   "outputs": [],
   "source": [
    "type(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dir(my_text):\n",
    "    if not item.startswith(\"__\"):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGs9PFIfKh78",
    "outputId": "bdb103a1-206e-4054-af2b-4ee3a38ec0ab"
   },
   "outputs": [],
   "source": [
    "# How many times does the word \"example\" appear?\n",
    "my_text.count(\"example\")\n",
    "\n",
    "# Notes:\n",
    "#  - my_text = our text, processed (loaded) by NLTK\n",
    "#     - technically: a Python object\n",
    "#  - my_text.count(...) = requesting the object to perform a .count(...) function and return the result\n",
    "#     - technically: calling a .count() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VB6V_3NjKh78",
    "outputId": "05d8b353-4eba-45ab-a4e0-faab9c719572"
   },
   "outputs": [],
   "source": [
    "# count works on tokens (full words in this case)\n",
    "my_text.count('exam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJRLD4ZHKh78",
    "outputId": "10c906c4-366f-4400-bded-5caa18918025"
   },
   "outputs": [],
   "source": [
    "'exam' in my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUHVkUsAKh79",
    "outputId": "b3b10749-0047-496e-ff04-afffbb448047"
   },
   "outputs": [],
   "source": [
    "'example' in my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnPc8tWVKh79"
   },
   "source": [
    "### Tokenizing\n",
    "\n",
    "Let's convert a text string into nltk.Text.\n",
    "First, we need to split it into tokens (to *tokenize* it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkzEe21WKh79",
    "outputId": "ebf7681f-f6c8-4ac5-cfc0-6d5ae6c85bd7"
   },
   "outputs": [],
   "source": [
    "# We need to download a package containing punctuation before we can tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jv1T6GloKh7-",
    "outputId": "e86c77fe-4dc9-43fa-b615-d06c49666436"
   },
   "outputs": [],
   "source": [
    "# Splitting text into tokens (words, ...) = tokenizing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "excerpt = \"NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\"\n",
    "tokens = word_tokenize(excerpt)\n",
    "\n",
    "tokens[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SLsB1DtPD78",
    "outputId": "1640556f-1b2d-4de1-c45b-c708eca147da"
   },
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6pxVMxTKh7-",
    "outputId": "4927a6d8-64b9-411c-c2fd-733c419bf5e7"
   },
   "outputs": [],
   "source": [
    "my_text2 = nltk.Text(tokens)\n",
    "\n",
    "print(my_text2.count(\"NLTK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EHoT6sWPPVx",
    "outputId": "6f2c8642-6f7a-4e29-cbc1-51fd4302fd98"
   },
   "outputs": [],
   "source": [
    "list(my_text2)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWmCHfY6Kh7-"
   },
   "source": [
    "### Downloading NLTK language resources\n",
    "\n",
    "NLTK also contains many language resources (corpora, ...) but you have select and download them separately (in order to save disk space and only download what is needed).\n",
    "\n",
    "Let's download text collections used in the NLTK book: \n",
    "* `nltk.download(\"book\")`\n",
    "\n",
    "Note: you can also download resources interactively:\n",
    "* `nltk.download()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUvRRRMBKh7-",
    "outputId": "33e61ccc-a8dc-4703-bd77-a0223a6db342"
   },
   "outputs": [],
   "source": [
    "# this is a big download of all book packages\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVTBPUs1Kh7_",
    "outputId": "2f0be272-159d-4de6-98df-264a784ce99e"
   },
   "outputs": [],
   "source": [
    "# After downloading the reources we still need to import them\n",
    "\n",
    "# Let's import all NLTK book resource (*)\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EwcXQNvKh7_"
   },
   "source": [
    "## 2) Exploring textual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.book.texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECtX8fzqKh7_",
    "outputId": "e68592d6-f2aa-4e9d-cb63-7f7fe4fa1c6e"
   },
   "outputs": [],
   "source": [
    "# text1, ... resources are of type nltk.Text (same as in the earlier example):\n",
    "\n",
    "type(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-HDe1sZKh7_",
    "outputId": "31b876e6-8007-4c7f-f0e1-f03e399a64bf"
   },
   "outputs": [],
   "source": [
    "# We can run all methods that nltk.Text has.\n",
    "\n",
    "# Count words:\n",
    "print(text1.count(\"whale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaK8BnPhPYlI"
   },
   "source": [
    "## Concordance\n",
    "\n",
    "The concordance has a long history in humanities study and Roberto Busa's concordance Index Thomisticus—started in 1946—is arguably the first digital humanities project. Before computers were common, they were printed in large volumes such as John Bartlett's 1982 reference book A Complete Concordance to Shakespeare—it was 1909 pages pages long! \n",
    "\n",
    "A concordance gives the context of a given word or phrase in a body of texts. For example, a literary scholar might ask: how often and in what context does Shakespeare use the phrase \"honest Iago\" in Othello? A historian might examine a particular politician's speeches, looking for examples of a particular \"dog whistle\".\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "\n",
    "* Geoffrey Rockwell and Stéfan Sinclair. [Tremendous Mechanical Labor: Father Busa's Algorithm](http://www.digitalhumanities.org/dhq/vol/14/3/000456/000456.html) (2020)\n",
    "* Julianne Nyhan and Marco Passarotti, eds. [One Origin of Digital Humanities: Fr Roberto Busa in His Own Words](https://www.amazon.com/One-Origin-Digital-Humanities-Roberto/dp/3030183114/) (2019)\n",
    "* Julianne Nyhan and Melissa Terras. [Uncovering 'hidden contributions to the history of Digital Humanities: the Index Thomisticus' femal keypunch operators](https://discovery.ucl.ac.uk/id/eprint/10052279/9/Nyhan_DH2017.redacted.pdf) (2017)\n",
    "* Steven E. Jones [Roberto Busa, S.J., and the Emergence of Humanities Computing](https://www.routledge.com/Roberto-Busa-S-J-and-the-Emergence-of-Humanities-Computing-The-Priest/Jones/p/book/9781138587250) (2016)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBdyd_r1Kh7_",
    "outputId": "2b241f36-0f15-4d88-8be4-3d0f45cf8213"
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.html#nltk.text.Text.concordance\n",
    "\n",
    "# Print concordance view (occurences of a word, in context):\n",
    "text1.concordance(\"discover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAd3tH8sKh8A",
    "outputId": "92b96d2f-ca73-4976-d73f-4b7d0a11a101"
   },
   "outputs": [],
   "source": [
    "text4.concordance(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DaGeHZYKh8A",
    "outputId": "ee94c713-7cf9-4bc0-fae9-69c13a657cdd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.html#nltk.text.Text.similar\n",
    "\n",
    "# Print words that appear in similar context as \"nation\".\n",
    "text4.similar(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(text4.similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RChhUPlDKh8A",
    "outputId": "56ee4bd7-a900-4a40-dca6-a5bab6b1879b"
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.html#nltk.text.Text.common_contexts\n",
    "\n",
    "# Find contexts common to all given words\n",
    "text1.common_contexts([\"day\", \"night\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(text1.common_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.collocations(num=40, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(text4.collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbwda6BlKh8A",
    "outputId": "77ac1711-ca0e-45ca-c420-118d3e3cfe64"
   },
   "outputs": [],
   "source": [
    "# nltk.Text is also a list - can do everything we can do with lists (access parts of it, ...)\n",
    "\n",
    "# What's the 1st occurence of \"He\" in the text?\n",
    "#  - note: Python is case sensitive (unless you take care of it - e.g. convert all text to lowercase)\n",
    "\n",
    "print(text1.index(\"He\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExM81vXJKh8A",
    "outputId": "3594b111-5f6c-4af0-d629-b91cd7bb55f4"
   },
   "outputs": [],
   "source": [
    "# The word at position #42\n",
    "#  - note: list indexes start from 0\n",
    "\n",
    "print(text1[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drMJLukzKh8C",
    "outputId": "e106861c-d7e9-4f8b-c9cb-ed6516c56800"
   },
   "outputs": [],
   "source": [
    "print(text1[42:52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2nrOBZjKh8C"
   },
   "source": [
    "## Visualizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "cPtzGbhGKh8C",
    "outputId": "a89c4113-8ce4-4e47-c348-a178e999b0fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dispersion plot\n",
    "\n",
    "# source: Inaugural Address Corpus\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"duty\", \"freedom\", \"America\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DKY-LjYKh8C",
    "outputId": "2ce28316-8868-4165-b883-d562ebefd0f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(text4.dispersion_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency plot\n",
    "\n",
    "text4.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(FreqDist.plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71CtOpUcLrph"
   },
   "source": [
    "## Converting Our Corpora into a NLTK Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blikIpXvLqYR",
    "outputId": "b85c8e00-6afc-4b11-fee8-8cd082283f20"
   },
   "outputs": [],
   "source": [
    "# We can use Pandas to read tabular data from any publicly accessible source\n",
    "url = \"https://github.com/CaptSolo/BSSDH_2023_beginners/raw/main/corpora/en_old_newspapers_5k.tsv\"\n",
    "\n",
    "## Another file we could have used:\n",
    "# url_2 = \"https://github.com/CaptSolo/BSSDH_2023_beginners/raw/main/corpora/lv_old_newspapers_5k.tsv\"\n",
    "\n",
    "# Read a CSV file and put its contents in a Pandas dataframe\n",
    "df = pd.read_csv(url, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape (size) of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "vckh__2qM3G8",
    "outputId": "f0c5453f-b300-4edd-c595-11a10ad46572"
   },
   "outputs": [],
   "source": [
    "# First 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Itqdnsz6M80z",
    "outputId": "a36aff65-6066-467b-c10e-fd360fc4bb6a"
   },
   "outputs": [],
   "source": [
    "# Let us sort by Date - even though it is a string type\n",
    "\n",
    "df = df.sort_values(by=\"Date\", ascending=True)  # ascending is True by default, if you wanted Descending you could use ascending=False\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5JPnHPNNdRo"
   },
   "source": [
    "### Extracting Text from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV0TktGUNYa_",
    "outputId": "f71555b9-55e5-48ce-ab40-cc8c80f4c753"
   },
   "outputs": [],
   "source": [
    "### Extracting the Text column from the dataframe\n",
    "\n",
    "documents = list(df.Text)  # df[\"Text\"].tolist() would do the same\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ftwKgGZVKUg",
    "outputId": "5843110e-c3f9-459e-d190-4a8a21eaad9d"
   },
   "outputs": [],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN_MVv2PN1Xb",
    "outputId": "75d2bc0e-4c6a-495e-f092-7bb1079dd2c4"
   },
   "outputs": [],
   "source": [
    "documents[:3]  # first three documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLLGPiE4OBs8",
    "outputId": "93c8be29-c8ae-4992-9884-3ac6a1a94b99"
   },
   "outputs": [],
   "source": [
    "# for the purpose of this analysis we will join all the documents together \n",
    "# this is not always appropriate depending on your needs\n",
    "\n",
    "all_docs = \"\\n\".join(documents)\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "1ek62y91OQLs",
    "outputId": "1455bc10-a907-4610-bd2f-f0f12645171d"
   },
   "outputs": [],
   "source": [
    "all_docs[:120] # so now all documents are in one big string \n",
    "# notice the \\n indicating newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ziR_UYJQKJs"
   },
   "source": [
    "Next, we lowercase our text and use the Natural Language Toolkit (NLTK) to tokenize it. Tokenizing breaks up the the document into individual words. Finally, we use our tokens to create an NLTK Text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOfqusj6QQfj",
    "outputId": "06d4bc03-7a76-42a6-c0b6-59edb2659a5a"
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "import nltk  # not needed if you already imported\n",
    "nltk.download('punkt')  # again not needed if you already downloaded punkt\n",
    "\n",
    "file_contents = all_docs.lower()\n",
    "tokens = nltk.word_tokenize(file_contents)\n",
    "\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtBcbq1eQbtr",
    "outputId": "ea7fe910-91b5-4f77-da72-1f165a70ed26"
   },
   "outputs": [],
   "source": [
    "# Verify that we have created an NLTK Text object\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fd4bZGsHQd28",
    "outputId": "7976c45e-3b03-4991-fd49-7eee354bccb4"
   },
   "outputs": [],
   "source": [
    "# Create a concordance for the given word\n",
    "text.concordance('million')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewhALtehQt7B"
   },
   "source": [
    "By default, the first 25 matches are printed along with 80 characters on each side of our string text. We can specify that more lines should be shown using a `lines` and `width` argument that accept integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2uJ1SoCQvOq",
    "outputId": "cf901b7f-f64a-4fea-df9d-15ef650b8099"
   },
   "outputs": [],
   "source": [
    "# Create a concordance for the given word\n",
    "# Increasing lines shown and number of characters\n",
    "text.concordance('school', lines=50, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwzZJ-47Q9Ed"
   },
   "source": [
    "If we want to supply a bigram, trigram, or longer construction, they are supplied as individual strings within a Python list. (If you try to supply a string with a space in the middle, there will be no results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Srxv5oxCQ5IP",
    "outputId": "7eded3ae-b967-4bbb-8ec8-76b9e34547b1"
   },
   "outputs": [],
   "source": [
    "# Create a concordance for a sequence of words\n",
    "text.concordance(['high', 'school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKbOvlF1WS5w",
    "outputId": "5069d027-6747-46b5-9bc5-b41d84d2dec4"
   },
   "outputs": [],
   "source": [
    "text.concordance('high school')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1iSikW6ROoR"
   },
   "source": [
    "This method works well for a quick preview of the lines, but if we want to save this concordance for later analysis we can use the `.concordance_list()` method. The `.concordance_list()` method outputs a list, but the elements of that list *are not* simple strings. They are ConcordanceLine objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI3V-jfLRQen"
   },
   "outputs": [],
   "source": [
    "# Output the concordance data\n",
    "output_list = text.concordance_list(['high', 'school'], width=200, lines=50)  # we do not have 50 matches in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7VqhAHkCRHb8",
    "outputId": "32ca1189-dfe0-4166-dbf1-95f67903dad3"
   },
   "outputs": [],
   "source": [
    "type(output_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "tJC-ZB9LRrzw",
    "outputId": "e6715f2e-3a61-42f6-ec70-539092b15b30"
   },
   "outputs": [],
   "source": [
    "# We can view individual lines by using a Python list index followed by .line.\n",
    "output_list[0].line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2XTeADP_Wygw",
    "outputId": "d7a20ea6-4c81-40f3-c802-f082b593ba88"
   },
   "outputs": [],
   "source": [
    "output_list[0].query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEOUvXeZR3Ph"
   },
   "source": [
    "If we want to save our concordance, we can write to a file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9-09WRYR2er"
   },
   "outputs": [],
   "source": [
    "# Writing the concordance to a text file\n",
    "\n",
    "# encoding=\"utf-8\" is very important for languages using symbols outside regular English characters\n",
    "with open('my_concordance.txt', mode='w',encoding=\"utf-8\") as f:\n",
    "    \n",
    "    for row in output_list:\n",
    "        f.write(row.line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are using Google Colab, it is important to download saved files!\n",
    "\n",
    "Files saved to the local directory (e.g. my_concordance.txt) of Google Colab servers will disappear after some time. In order to save these files, download them.\n",
    "\n",
    "Another alternative is to save files to Google Drive folders in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3WrsuE-LLho"
   },
   "source": [
    "## Normalizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3a9rLvkLLhq"
   },
   "source": [
    "**Normalizing text is covered in NLTK Book section 3.6:**\n",
    "https://www.nltk.org/book/ch03.html#sec-normalizing-text\n",
    "\n",
    "Yesterday we already normalized / converted text to lowercase but often that's not enough. \n",
    "\n",
    "We may want to go further and strip off any affixes, a task known as *stemming*. A further step is to make sure that the resulting form is a known word in a dictionary, a task known as *lemmatization*. \n",
    "\n",
    "NLTK offers some stemming and lemmatization functions but they are limited to just some languages (e.g. Latvian is not one of them).\n",
    " \n",
    "https://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjzPW0TkLLhr",
    "outputId": "3b41d803-ef6c-4767-91f4-111bbd0f45f6"
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for t in tokens:\n",
    "    print(porter.stem(t), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uZrxsAaZmWd",
    "outputId": "01f40287-1280-4159-85ce-37b47bf74f2b"
   },
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5FUvrqzLLht",
    "outputId": "6e85b84c-e5b1-4db3-e7c6-d35a5a8a6e77"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for t in tokens:\n",
    "    print(wnl.lemmatize(t), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Working with languages not supported by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/CaptSolo/BSSDH_2023_beginners/raw/main/corpora/lv_old_newspapers_5k.tsv\"\n",
    "\n",
    "# Read a CSV file and put its contents in a Pandas dataframe\n",
    "df = pd.read_csv(url, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"Date\", ascending=True)  # ascending is True by default, if you wanted Descending you could use ascending=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create NLTK Text\n",
    "\n",
    "documents = list(df.Text)\n",
    "all_docs = \"\\n\".join(documents)\n",
    "\n",
    "file_contents = all_docs.lower()\n",
    "tokens = nltk.word_tokenize(file_contents)\n",
    "\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.concordance(\"basketbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how frequently does \"basketbols\" appear in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1ueub-NLLho",
    "outputId": "8203d0e0-7173-45f9-b470-9c793d0f2731"
   },
   "outputs": [],
   "source": [
    "for item in text:\n",
    "    if item == \"basketbols\":\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ED3hSHhLLhp",
    "outputId": "a35dec49-14f8-47ef-aac3-b2fd7f803df9"
   },
   "outputs": [],
   "source": [
    "# There are other variations of the same word in the text:\n",
    "\n",
    "import regex\n",
    "\n",
    "for item in text:\n",
    "    if regex.match(r\"basket.*\", item):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0cqJFhkLLhu"
   },
   "source": [
    "If the language of our text is not supported by NLTK we can use another library: \n",
    "\n",
    "simplemma: https://pypi.org/project/simplemma/\n",
    "\n",
    "More information: [A simple multilingual lemmatizer for Python](https://adrien.barbaresi.eu/blog/simple-multilingual-lemmatizer-python.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN-a1B_WLLhv",
    "outputId": "4ab6752e-86ef-4982-ad43-b44dd5bcc9e0"
   },
   "outputs": [],
   "source": [
    "# we may need to install simplemma first (uncomment the following line to do that)\n",
    "!pip install simplemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y63TrZpuLLhw"
   },
   "outputs": [],
   "source": [
    "import simplemma as sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jwb8_UOJLLhw",
    "outputId": "b558646f-8d1c-400b-d179-41dedfe313a7"
   },
   "outputs": [],
   "source": [
    "buf = []\n",
    "\n",
    "for item in text:\n",
    "    buf.append(sl.lemmatize(item, lang=\"lv\"))\n",
    "               \n",
    "buf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGtS-LlyLLhx"
   },
   "outputs": [],
   "source": [
    "text_new = nltk.Text(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "je417o6OdAVr",
    "outputId": "f4ee976b-9806-4ba8-99b8-198f34cdd8fd"
   },
   "outputs": [],
   "source": [
    "text_new.concordance(\"basketbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA4CMUHWKh8D"
   },
   "source": [
    "---\n",
    "\n",
    "## Your turn!\n",
    "\n",
    "Choose an NLTK text corpus and **explore it using NLTK** (following the examples in this notebook).\n",
    "\n",
    "**Write code in notebook cells below**.\n",
    "* add more cells (use \"+\" icon) if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Parts of this notebook were adopted from notebook by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLTK_Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
