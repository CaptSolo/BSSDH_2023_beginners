{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd01521",
   "metadata": {
    "id": "abd01521"
   },
   "source": [
    "# Reading Data, Pandas\n",
    "\n",
    "There are various file formats, how do we make a sense of them all?\n",
    "\n",
    "* There are archive/compression formats such as .zip, .rar, .7z, .tar those hold other files.\n",
    "* There are text formats such as .txt, .csv, .json, .tsv - those can be read by humans in a text editor\n",
    "* There are binary formats such as .exe, .jpg, .png - those are not human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf08d1",
   "metadata": {
    "id": "06bf08d1"
   },
   "source": [
    "### Reading text files\n",
    "\n",
    "In this section we will read a simple text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac177026",
   "metadata": {
    "id": "ac177026"
   },
   "outputs": [],
   "source": [
    "filename = \"alice_wonderland.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88c700",
   "metadata": {},
   "source": [
    "The following two cells are commented out because they might not work in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdded79",
   "metadata": {
    "id": "0bdded79"
   },
   "outputs": [],
   "source": [
    "## open the file in current directory for reading\n",
    "#file_1 = open(filename)\n",
    "\n",
    "## read contents of the file\n",
    "#data = file_1.read()\n",
    "\n",
    "## close the file\n",
    "#file_1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a better way (automatically closing the open file)\n",
    "\n",
    "#with open(filename) as file_1:   \n",
    "#    data = file_1.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b67e7",
   "metadata": {
    "id": "0c5b67e7"
   },
   "source": [
    "### Google Colab\n",
    "\n",
    "Note: The above action (reading a local file) will fail if you execute it in Google Colab. \n",
    "\n",
    "We can open it from a remote web location (from Github) instead. Let's use the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db22d13",
   "metadata": {
    "id": "2db22d13"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/notebooks/\" + filename\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a8bd4",
   "metadata": {},
   "source": [
    "### Let's continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c83bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56c83bfd",
    "outputId": "1e38cbc2-253f-4b16-f123-786160c1c7df"
   },
   "outputs": [],
   "source": [
    "# print the first 100 characters of the file\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b91449",
   "metadata": {
    "id": "30b91449"
   },
   "outputs": [],
   "source": [
    "# split text into tokens (words)\n",
    "words = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4c041",
   "metadata": {
    "id": "e6a4c041",
    "outputId": "20d1943f-439b-4638-8dd6-4ae12cb9a963"
   },
   "outputs": [],
   "source": [
    "# count the number of tokens in text\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696dd23",
   "metadata": {
    "id": "6696dd23",
    "outputId": "417a5525-8f4c-4237-a709-2e4a4407f65d"
   },
   "outputs": [],
   "source": [
    "# print the first 50 tokens\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f41a77",
   "metadata": {
    "id": "63f41a77"
   },
   "source": [
    "### Counting word frequency\n",
    "\n",
    "Here we will use Python's Counter object (from Python collections library) to determine word frequency of the text. \n",
    "\n",
    "https://docs.python.org/3/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3391228",
   "metadata": {
    "id": "e3391228"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac286d47",
   "metadata": {
    "id": "ac286d47"
   },
   "outputs": [],
   "source": [
    "c = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41935a",
   "metadata": {
    "id": "2b41935a",
    "outputId": "5a49e01f-f2f7-4dcb-c40e-be279e075fc6"
   },
   "outputs": [],
   "source": [
    "# print the 20 most common words (tokens)\n",
    "print(c.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34be2e",
   "metadata": {
    "id": "de34be2e",
    "outputId": "21e9bd65-9631-4839-989d-22781b51b0ae"
   },
   "outputs": [],
   "source": [
    "# a nicer way of printing counter results using a *for* cycle\n",
    "\n",
    "for token, count in c.most_common(20):\n",
    "    print(f\"{token}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf4f58",
   "metadata": {
    "id": "5ecf4f58"
   },
   "source": [
    "Notice how words may appear in both lowercase (\"the\") and uppercase (\"The\"). You may want to normalize the text by converting it all to lowercase and do other clean-up steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5005661",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Pandas lets us define `DataFrames` that contain tabular data organized in columns and rows:\n",
    "* both columns and rows may have labels (names for these columns / rows)\n",
    "* every column has its data type (different columns may have different data types)\n",
    "\n",
    "Pandas also lets us define `Series` that contain a series of data (one column). Every `Series` element may have a label (name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d1d0b",
   "metadata": {
    "id": "f49d1d0b"
   },
   "source": [
    "### Reading TSV files\n",
    "\n",
    "Corpora that we could work with are located in archived TSV (Tab-separated-values) files:\n",
    "https://github.com/CaptSolo/BSSDH_2023_beginners/tree/main/corpora\n",
    "\n",
    "These files consist of rows (records) that contain one or more values separated by \"Tab\" characters.\n",
    "\n",
    "We will use Pandas library to read a TSV file that contains a smaller version of the \"lv_old_newspapers.zip\" corpus: https://github.com/CaptSolo/BSSDH_2023_beginners/blob/main/corpora/lv_old_newspapers_5k.tsv\n",
    "\n",
    "You may also use a TSV file for an English newspaper corpus (with slightly different column names): https://github.com/CaptSolo/BSSDH_2023_beginners/blob/main/corpora/en_old_newspapers_5k.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194e455",
   "metadata": {
    "id": "5194e455"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# common alternative \n",
    "# import pandas as pd\n",
    "# this would let you save 4 characters each time you need some pandas functionality you would write pd instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b4e8f",
   "metadata": {
    "id": "787b4e8f"
   },
   "outputs": [],
   "source": [
    "# Commented out code that will not work in Google Colab\n",
    "\n",
    "## if you downloaded and unarchived the whole Github repository \n",
    "## this is where you will find the lv_old_newspapers_5k.tsv file:\n",
    "\n",
    "#filename = \"../corpora/lv_old_newspapers_5k.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694238e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "694238e0",
    "outputId": "935c4700-137f-42ec-fa7c-c1e40c60e63c"
   },
   "outputs": [],
   "source": [
    "## read the tab-separated file (\"sep\" parameter tells Pandas that values in the file\n",
    "## are separated with the \"tab\" character.\n",
    "\n",
    "#df_1 = pandas.read_csv(filename, sep=\"\\t\") # instead of df_1 we could use another name for our variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89655e67",
   "metadata": {
    "id": "89655e67"
   },
   "source": [
    "#### Google Colab\n",
    "\n",
    "Note: The above action (reading a local file) will fail if you execute it in Google Colab.\n",
    "\n",
    "We have two different approaches then:\n",
    "\n",
    "1. Upload file to Google Colab (remember this is temporary). Read it just like you would on a local computer. \n",
    "\n",
    "2. Download file(s) from web address, instead of file path we will use its web addrss (URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-lAt9XM54mF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q-lAt9XM54mF",
    "outputId": "fc5e2488-7ba7-4a3f-f32a-ebf5fa36cc81"
   },
   "outputs": [],
   "source": [
    "# Approach 1\n",
    "# Assuming file has been uploaded it will be found in current directory\n",
    "\n",
    "file_path = \"lv_old_newspapers_5k.tsv\"\n",
    "\n",
    "df_1 = pandas.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "# print the first lines of the file\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69605e00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "69605e00",
    "outputId": "bad16782-8d8c-4e76-fac2-5c8840917cb5"
   },
   "outputs": [],
   "source": [
    "# Approach 2 reading from a web address \n",
    "url = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/lv_old_newspapers_5k.tsv\"\n",
    "\n",
    "# ... or you could use the English corpus instead:\n",
    "# url = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/en_old_newspapers_5k.tsv\"\n",
    "\n",
    "df_2 = pandas.read_csv(url, sep=\"\\t\")\n",
    "\n",
    "# print the first lines of the file\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the basic statistics of the dataset\n",
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6aaa5",
   "metadata": {},
   "source": [
    "### Let's continue working with the dataframe (containing a text corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc2734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bcc2734",
    "outputId": "11ecb4bd-d0b8-4b93-e73e-e5f88d8ce9c9"
   },
   "outputs": [],
   "source": [
    "# the size of the corpus:\n",
    "print(len(df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5f4f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62c5f4f8",
    "outputId": "05a25c2e-5a39-4a67-a7ab-d786e2015231"
   },
   "outputs": [],
   "source": [
    "# select the Text column, show the first 10 entries\n",
    "\n",
    "df_1[\"Text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eBJq74i47f-N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBJq74i47f-N",
    "outputId": "0adaf4f7-9def-4a13-930e-b7d2eeb00dd4"
   },
   "outputs": [],
   "source": [
    "# we can get ALL of the text in one big string from a pandas column\n",
    "\n",
    "list_of_rows = list(df_1.Text)\n",
    "len(list_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aD-jSYH_7wMe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aD-jSYH_7wMe",
    "outputId": "4fe0f3b5-441d-4b91-8159-c29fa334f404"
   },
   "outputs": [],
   "source": [
    "# let's see what we have in first 3 rows\n",
    "list_of_rows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EA7Nd7sR71zq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "EA7Nd7sR71zq",
    "outputId": "1c647769-8bc3-476c-e1ed-bc0958ec8bc7"
   },
   "outputs": [],
   "source": [
    "all_text = \"\\n\".join(list_of_rows) # we can join all rows into one big string \n",
    "# separating each document with a newline, but you could choose something else to join with\n",
    "\n",
    "# \"\\n\" means a newline symbol\n",
    "\n",
    "all_text[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c46d3",
   "metadata": {
    "id": "e28c46d3"
   },
   "source": [
    "### Reading archived files\n",
    "\n",
    "Pandas can also read archived CSV and TSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0af606",
   "metadata": {
    "id": "3d0af606"
   },
   "outputs": [],
   "source": [
    "# filename_2 = \"../corpora/lv_old_newspapers.zip\"\n",
    "\n",
    "## read the archived, tab-separated file (\"compression\" parameter tells\n",
    "## Pandas that this is a ZIP archived file).\n",
    "\n",
    "# df_2 = pandas.read_csv(filename_2, sep=\"\\t\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c131a",
   "metadata": {
    "id": "589c131a"
   },
   "source": [
    "Note: The above action (reading a local file) that is commented out will fail if you execute it in Google Colab.\n",
    "\n",
    "We will use downloading from a remote web location instead (a Github repository in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccef3e2",
   "metadata": {
    "id": "cccef3e2"
   },
   "outputs": [],
   "source": [
    "url_2 = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/lv_old_newspapers.zip\"\n",
    "\n",
    "df_2 = pandas.read_csv(url_2, sep=\"\\t\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9587588",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9587588",
    "outputId": "ddff5277-83b1-4a33-fb15-3d23e692e405"
   },
   "outputs": [],
   "source": [
    "# the size of the corpus:\n",
    "\n",
    "print(len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525f55f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "6525f55f",
    "outputId": "ad09cbde-81b2-42f8-951d-055753794f9d"
   },
   "outputs": [],
   "source": [
    "# show the last 10 entries\n",
    "\n",
    "df_2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataset\n",
    "df_2.sort_values(by=[\"Date\"])\n",
    "\n",
    "# Minimum value\n",
    "df_2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0822def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum value\n",
    "df_2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_HsCDJ8i8gZc",
   "metadata": {
    "id": "_HsCDJ8i8gZc"
   },
   "source": [
    "##  Reading other formats\n",
    "\n",
    "Pandas supports a wide variety of file formats\n",
    "\n",
    "Full list of formats is available here: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "For example to read Excel files you would use my_dataframe = pandas.read_excel(filepath)\n",
    "where filepath would be a string with file location or web address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q4zGUWlX9TvF",
   "metadata": {
    "id": "Q4zGUWlX9TvF"
   },
   "source": [
    "## Task - read data into a dataframe from file\n",
    "\n",
    "We have 4 different corpora for you to use.\n",
    "\n",
    "Web addresses:\n",
    "\n",
    "* English - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/en_old_newspapers_5k.tsv\n",
    "* Estonian - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/ee_old_newspapers.zip\n",
    "* Latvian - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/lv_old_newspapers.zip\n",
    "* Ukrainian - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/ua_old_newspapers.zip\n",
    "\n",
    "Load one of them in a Pandas dataframe. Check the length, shape, sort them, see the first 15 entries and the last 20 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af637a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafb665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfc9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1cb4fb1",
   "metadata": {},
   "source": [
    "# Text Mining with NLTK and Pandas\n",
    "\n",
    "Source: [Text Mining and Sentiment Analysis with NLTK and Pandas in Python](https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/)\n",
    "* by Jan Kirenz\n",
    "* license: [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import some tweets from Barack Obama \n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/kirenz/twitter-tweepy/main/tweets-obama.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "\n",
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee1877",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "* We use NLTK's RegexpTokenizer to perform tokenization in combination with regular expressions\n",
    "  * `\\w+` matches Unicode word characters with one or more occurrences\n",
    "  * this includes most characters that can be part of a word in any language, as well as numbers and the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb316ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "df['text_token']=df['text'].apply(regexp.tokenize)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17440f53",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5efe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Extend the list with your own custom stopwords\n",
    "my_stopwords = ['https']\n",
    "stopwords.extend(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function to remove stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [item for item in words if item not in stopwords]\n",
    "\n",
    "# apply this function to every dataframe row:\n",
    "df['text_token'] = df['text_token'].apply(remove_stopwords)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24c67f",
   "metadata": {},
   "source": [
    "### Remove infrequent words\n",
    "\n",
    "We first change the format of text_token to strings and keep only words which are longer than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04493f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for joining together words longer than 2 letters\n",
    "def join_words(words):\n",
    "    return ' '.join([item for item in words if len(item)>2])\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df['text_string'] = df['text_token'].apply(join_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ecdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'text_token', 'text_string']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e212f8",
   "metadata": {},
   "source": [
    "### Continue working with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words\n",
    "all_words = ' '.join([word for word in df['text_string']])\n",
    "\n",
    "# Tokenize all_words\n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokenized_words)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns words that appear more than once\n",
    "\n",
    "def freq_words(words):\n",
    "    return ' '.join([item for item in words if fdist[item] >= 1 ])\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df['text_string_fdist'] = df['text_token'].apply(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'text_string', 'text_string_fdist']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddc0f0",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We used Pandas to hold data of a Tweet message corpus as it went through transformations: tokenization, stopword removal, etc.\n",
    "* after each transformation we added a new Pandas dataframe column to hold the transformed data\n",
    "\n",
    "Further steps:\n",
    "* see how to visualize data in the `Day 2 - Visualization` Jupyter notebook;\n",
    "* see the [Text Mining and Sentiment Analysis with NLTK and Pandas in Python](https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae6fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a24ea476",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook by Uldis Bojārs is available under the [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) license."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Reading_Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
